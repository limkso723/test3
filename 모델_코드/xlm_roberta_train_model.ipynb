{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f4gZ2Z3FNbqH","outputId":"1a436e82-897d-4345-9ee5-8fdb2eb81f73","executionInfo":{"status":"ok","timestamp":1667961088167,"user_tz":-540,"elapsed":18152,"user":{"displayName":"Python Team","userId":"00791902300481953740"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_RDH-YQsQ2K0","outputId":"7ec7638e-bf5d-4840-d9b5-553a6348b464","executionInfo":{"status":"ok","timestamp":1668153111819,"user_tz":-540,"elapsed":14243,"user":{"displayName":"Python Team","userId":"00791902300481953740"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers==4.24.0\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 5.4 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (3.8.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (21.3)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.10.1-py3-none-any.whl (163 kB)\n","\u001b[K     |████████████████████████████████| 163 kB 55.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (1.21.6)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 39.6 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (2022.6.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (4.64.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (6.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.24.0) (4.13.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers==4.24.0) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.24.0) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.24.0) (3.10.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.24.0) (2022.9.24)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.10.1 tokenizers-0.13.2 transformers-4.24.0\n"]}],"source":["# 필요 시 설치(가상환경의 경우 터미널에 설치)\n","!pip install transformers==4.24.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gmtfOHQSQ-nS","outputId":"1abeeb9a-5e56-4438-9c3c-2c92c2588df7","executionInfo":{"status":"ok","timestamp":1668153137627,"user_tz":-540,"elapsed":14441,"user":{"displayName":"Python Team","userId":"00791902300481953740"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets==2.6.1\n","  Downloading datasets-2.6.1-py3-none-any.whl (441 kB)\n","\u001b[K     |████████████████████████████████| 441 kB 5.3 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (6.0)\n","Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (6.0.1)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (4.64.1)\n","Collecting dill<0.3.6\n","  Downloading dill-0.3.5.1-py2.py3-none-any.whl (95 kB)\n","\u001b[K     |████████████████████████████████| 95 kB 4.3 MB/s \n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.1.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[K     |████████████████████████████████| 212 kB 43.4 MB/s \n","\u001b[?25hCollecting multiprocess\n","  Downloading multiprocess-0.70.14-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 49.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (2.23.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (1.3.5)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (2022.10.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (4.13.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (21.3)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (3.8.3)\n","Collecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from datasets==2.6.1) (0.10.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (6.0.2)\n","Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (4.1.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (1.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (1.8.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (4.0.2)\n","Requirement already satisfied: asynctest==0.13.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (0.13.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (1.3.3)\n","Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (2.1.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->datasets==2.6.1) (22.1.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets==2.6.1) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets==2.6.1) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.6.1) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.6.1) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.6.1) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets==2.6.1) (1.24.3)\n","Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 43.0 MB/s \n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->datasets==2.6.1) (3.10.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.13-py37-none-any.whl (115 kB)\n","\u001b[K     |████████████████████████████████| 115 kB 43.5 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==2.6.1) (2.8.2)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets==2.6.1) (2022.6)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets==2.6.1) (1.15.0)\n","Installing collected packages: urllib3, dill, xxhash, responses, multiprocess, datasets\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: dill\n","    Found existing installation: dill 0.3.6\n","    Uninstalling dill-0.3.6:\n","      Successfully uninstalled dill-0.3.6\n","Successfully installed datasets-2.6.1 dill-0.3.5.1 multiprocess-0.70.13 responses-0.18.0 urllib3-1.25.11 xxhash-3.1.0\n"]}],"source":["# 필요 시 설치(가상환경의 경우 터미널에 설치)\n","!pip install datasets==2.6.1"]},{"cell_type":"markdown","metadata":{"id":"jprcZhTyetFF"},"source":["# 모듈 import 및 전역 변수 설정"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8LDwADWNMpsC"},"outputs":[],"source":["import json\n","import os\n","\n","import torch\n","import torch.nn as nn\n","from tqdm import trange\n","from transformers import XLMRobertaModel, AutoTokenizer, AutoModel\n","from torch.utils.data import DataLoader, TensorDataset\n","from transformers import get_linear_schedule_with_warmup\n","from transformers import AdamW\n","from datasets import load_metric\n","from sklearn.metrics import f1_score\n","import pandas as pd\n","import copy\n","\n","PADDING_TOKEN = 1\n","S_OPEN_TOKEN = 0\n","S_CLOSE_TOKEN = 2\n","\n","do_eval=True\n","\n","# 학습된 모델이 저장될 경로 설정\n","category_extraction_model_path = ''\n","polarity_classification_model_path = ''\n","\n","# 모델을 이어서 학습할 시 불러올 파일 경로 설정\n","test_category_extraction_model_path = ''\n","test_polarity_classification_model_path = ''\n","\n","# 학습할 json 데이터 설정\n","train_data_path = ''\n","# 학습 시 evaluation할 json 데이터 설정\n","dev_data_path = ''\n","# 학습 후 저장된 모델(pt file)로 predict파일을 만들 json 데이터 설정\n","test_data_path = ''\n","\n","# 최대 단어 수 설정\n","max_len = 256\n","# 배치 사이즈 설정\n","batch_size = 8\n","# 학습시 사용할 베이스 모델 설정\n","base_model = 'xlm-roberta-base'\n","# 러닝 레이트 설정\n","learning_rate = 3e-6\n","# eps 설정\n","eps = 1e-8\n","# 학습 할 에폭 수 설정\n","num_train_epochs = 20\n","classifier_hidden_size = 768\n","classifier_dropout_prob = 0.1\n","\n","# 개체#속성 쌍 설정(label25)\n","entity_property_pair = [\n","    '제품 전체#일반', '제품 전체#가격', '제품 전체#디자인', '제품 전체#품질', '제품 전체#편의성', '제품 전체#인지도', '제품 전체#다양성',\n","    '본품#일반', '본품#디자인', '본품#품질', '본품#편의성', '본품#다양성', '본품#가격', '본품#인지도',\n","    '패키지/구성품#일반', '패키지/구성품#디자인', '패키지/구성품#품질', '패키지/구성품#편의성', '패키지/구성품#가격', '패키지/구성품#다양성',\n","    '브랜드#일반', '브랜드#가격', '브랜드#품질', '브랜드#인지도', '브랜드#디자인',\n","                    ]\n","\n","# 개체#속성 쌍 설정(label23)\n","# entity_property_pair = [\n","#     '제품 전체#일반', '제품 전체#가격', '제품 전체#디자인', '제품 전체#품질', '제품 전체#편의성', '제품 전체#인지도',\n","#     '본품#일반', '본품#디자인', '본품#품질', '본품#편의성', '본품#다양성', '본품#가격', '본품#인지도',\n","#     '패키지/구성품#일반', '패키지/구성품#디자인', '패키지/구성품#품질', '패키지/구성품#편의성', '패키지/구성품#가격', '패키지/구성품#다양성',\n","#     '브랜드#일반', '브랜드#가격', '브랜드#품질', '브랜드#인지도',\n","#                     ]\n","\n","# 문장과 개체#속성 쌍의 관계를 True, False 로 표시\n","tf_id_to_name = ['True', 'False']\n","tf_name_to_id = {tf_id_to_name[i]: i for i in range(len(tf_id_to_name))}\n","\n","# 문장과 개체#속성 쌍의 관계로 감성을 positive, negative, neutral 로 표시\n","polarity_id_to_name = ['positive', 'negative', 'neutral']\n","polarity_name_to_id = {polarity_id_to_name[i]: i for i in range(len(polarity_id_to_name))}\n","\n","# 그래픽 카드 사용하는 변수\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# 불러온 tokenizer 에 special token 을 추가\n","special_tokens_dict = {\n","    'additional_special_tokens': ['&name&', '&affiliation&', '&social-security-num&', '&tel-num&', '&card-num&', '&bank-account&', '&num&', '&online-account&']\n","}"]},{"cell_type":"markdown","metadata":{"id":"xGmH15hCeqhJ"},"source":["json 및 jsonl 파일 read, write 함수"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6vGeHU4yP2Sg"},"outputs":[],"source":["# json 파일 읽어서 list에 저장\n","def jsonload(fname, encoding=\"utf-8\"):\n","    with open(fname, encoding=encoding) as f:\n","        j = json.load(f)\n","    return j\n","\n","# json 개체를 파일이름으로 저장\n","def jsondump(j, fname):\n","    with open(fname, \"w\", encoding=\"UTF8\") as f:\n","        json.dump(j, f, ensure_ascii=False)\n","\n","# jsonl 파일 읽어서 list에 저장\n","def jsonlload(fname, encoding=\"utf-8\"):\n","    json_list = []\n","    with open(fname, encoding=encoding) as f:\n","        for line in f.readlines():\n","            json_list.append(json.loads(line))\n","    return json_list"]},{"cell_type":"markdown","metadata":{"id":"xuHV8HGqXvg_"},"source":["# 모델 정의\n","xlm-roberta 모델을 기반으로 한 classification 모델 이용"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WkyrAEhAQBQV"},"outputs":[],"source":["# 아래 RoBertaBaseClassifier의 classifier로 사용될 class\n","class SimpleClassifier(nn.Module):\n","\n","    def __init__(self, num_label):\n","        super().__init__()\n","        self.dense = nn.Linear(classifier_hidden_size, classifier_hidden_size)\n","        self.dropout = nn.Dropout(classifier_dropout_prob)\n","        self.output = nn.Linear(classifier_hidden_size, num_label)\n","\n","    def forward(self, features):\n","        x = features[:, 0, :]\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.output(x)\n","        return x\n","\n","# 불러올 base model 기반 classification class 생성\n","class RoBertaBaseClassifier(nn.Module):\n","    def __init__(self, num_label, len_tokenizer):\n","        super(RoBertaBaseClassifier, self).__init__()\n","\n","        self.num_label = num_label\n","        self.xlm_roberta = XLMRobertaModel.from_pretrained(base_model)\n","        self.xlm_roberta.resize_token_embeddings(len_tokenizer)\n","\n","        self.labels_classifier = SimpleClassifier(self.num_label)\n","\n","    def forward(self, input_ids, attention_mask, labels=None):\n","        outputs = self.xlm_roberta(\n","            input_ids=input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=None\n","        )\n","\n","        sequence_output = outputs[0]\n","        logits = self.labels_classifier(sequence_output)\n","\n","        loss = None\n","\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss()\n","            loss = loss_fct(logits.view(-1, self.num_label),\n","                                                labels.view(-1))\n","\n","        return loss, logits\n"]},{"cell_type":"markdown","metadata":{"id":"q-p0LKeGi194"},"source":["# 데이터 파싱 및 tokenization 함수 정의\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XO-hv7lQQGA5"},"outputs":[],"source":["# 데이터를 tokenizer화\n","def tokenize_and_align_labels(tokenizer, form, annotations, max_len):\n","\n","    # entity_property 데이터를 넣을 빈 딕셔너리 생성\n","    entity_property_data_dict = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'label': []\n","    }\n","    # polarity 데이터를 넣을 빈 딕셔너리 생성\n","    polarity_data_dict = {\n","        'input_ids': [],\n","        'attention_mask': [],\n","        'label': []\n","    }\n","\n","    # 미리 만들어 둔 entity_property_pair 리스트 내에 '속성#개체' 쌍에 대하여 각각 tokenizer 후 label 달아줌\n","    for pair in entity_property_pair:\n","        isPairInOpinion = False\n","        if pd.isna(form):\n","            break\n","        tokenized_data = tokenizer(form, pair, padding='max_length', max_length=max_len, truncation=True)\n","        for annotation in annotations:\n","            entity_property = annotation[0]\n","            polarity = annotation[2]\n","\n","            if polarity == '------------':\n","                continue\n","\n","            # entity_property 는 True 일 때 polarity 도 함께 tonkenizer 후 label 달아줌\n","            if entity_property == pair:\n","                entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n","                entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n","                entity_property_data_dict['label'].append(tf_name_to_id['True'])\n","\n","                polarity_data_dict['input_ids'].append(tokenized_data['input_ids'])\n","                polarity_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n","                polarity_data_dict['label'].append(polarity_name_to_id[polarity])\n","\n","                isPairInOpinion = True\n","                break\n","\n","        # entity_property 는 False 일 때 polarity 는 작업하지 않음\n","        if isPairInOpinion is False:\n","            entity_property_data_dict['input_ids'].append(tokenized_data['input_ids'])\n","            entity_property_data_dict['attention_mask'].append(tokenized_data['attention_mask'])\n","            entity_property_data_dict['label'].append(tf_name_to_id['False'])\n","\n","    return entity_property_data_dict, polarity_data_dict\n","\n","# 데이터 셋 형성 함수\n","def get_dataset(raw_data, tokenizer, max_len):\n","    # tokenizer 된 entity_property 데이터가 들어갈 빈 리스트 생성 \n","    input_ids_list = []\n","    attention_mask_list = []\n","    token_labels_list = []\n","\n","    # tokenizer 된 polarity 데이터가 들어갈 빈 리스트 생성 \n","    polarity_input_ids_list = []\n","    polarity_attention_mask_list = []\n","    polarity_token_labels_list = []\n","\n","    # 문장별로 tokenize_and_align_labels 를 통해 tokenizer 후 각각의 리스트에 입력\n","    for utterance in raw_data:\n","        entity_property_data_dict, polarity_data_dict = tokenize_and_align_labels(tokenizer, utterance['sentence_form'], utterance['annotation'], max_len)\n","        input_ids_list.extend(entity_property_data_dict['input_ids'])\n","        attention_mask_list.extend(entity_property_data_dict['attention_mask'])\n","        token_labels_list.extend(entity_property_data_dict['label'])\n","\n","        polarity_input_ids_list.extend(polarity_data_dict['input_ids'])\n","        polarity_attention_mask_list.extend(polarity_data_dict['attention_mask'])\n","        polarity_token_labels_list.extend(polarity_data_dict['label'])\n","\n","    return TensorDataset(torch.tensor(input_ids_list), torch.tensor(attention_mask_list),\n","                         torch.tensor(token_labels_list)), TensorDataset(torch.tensor(polarity_input_ids_list), torch.tensor(polarity_attention_mask_list),\n","                         torch.tensor(polarity_token_labels_list))\n","\n"]},{"cell_type":"markdown","metadata":{"id":"5Ka98lsxi--Y"},"source":["# 모델 학습"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YL0yp7zSaX9B"},"outputs":[],"source":["# f1 score 를 통해 성능을 확인하는 함수\n","def evaluation(y_true, y_pred, label_len):\n","    count_list = [0]*label_len\n","    hit_list = [0]*label_len\n","    for i in range(len(y_true)):\n","        count_list[y_true[i]] += 1\n","        if y_true[i] == y_pred[i]:\n","            hit_list[y_true[i]] += 1\n","    acc_list = []\n","\n","    for i in range(label_len):\n","        acc_list.append(hit_list[i]/count_list[i])\n","\n","    print(count_list)\n","    print(hit_list)\n","    print(acc_list)\n","    print('accuracy: ', (sum(hit_list) / sum(count_list)))\n","    print('macro_accuracy: ', sum(acc_list) / 3)\n","    # print(y_true)\n","\n","    y_true = list(map(int, y_true))\n","    y_pred = list(map(int, y_pred))\n","\n","    print('f1_score: ', f1_score(y_true, y_pred, average=None))\n","    print('f1_score_micro: ', f1_score(y_true, y_pred, average='micro'))\n","    print('f1_score_macro: ', f1_score(y_true, y_pred, average='macro'))\n","\n","# 모델을 학습시키는 함수\n","def train_sentiment_analysis():\n","\n","    print('train_sentiment_analysis')\n","    print('category_extraction model would be saved at ', category_extraction_model_path)\n","    print('polarity model would be saved at ', polarity_classification_model_path)\n","\n","    print('loading train data')\n","    # train 데이터가 jsonl일 경우 위 코드 그대로 json일 경우는 아래 주석처리된 코드로 바꿔서 사용\n","    train_data = jsonlload(train_data_path)\n","    # train_data = jsonload(train_data_path)\n","\n","    # dev 데이터가 jsonl일 경우 위 코드 그대로 json일 경우는 아래 주석처리된 코드로 바꿔서 사용    \n","    dev_data = jsonlload(dev_data_path)\n","    # dev_data = jsonload(dev_data_path)\n","\n","    print('tokenizing train data')\n","    # pretrained base model tokenizer 불러옴\n","    tokenizer = AutoTokenizer.from_pretrained(base_model)\n","    # tokenizer 에 추가할 special token 설정\n","    num_added_toks = tokenizer.add_special_tokens(special_tokens_dict)\n","    print('We have added', num_added_toks, 'tokens')\n","    # get_dataset 함수 를 통해 entity_property, polarity train 데이터셋을 생성\n","    entity_property_train_data, polarity_train_data = get_dataset(train_data, tokenizer, max_len)\n","    # get_dataset 함수 를 통해 entity_property, polarity dev 데이터셋을 생성\n","    entity_property_dev_data, polarity_dev_data = get_dataset(dev_data, tokenizer, max_len)\n","    # DataLoader 를 통해 위에서 만든 entity_property_train_data 를 배치 사이즈 수 단위로 나누고, 데이터를 셔플\n","    entity_property_train_dataloader = DataLoader(entity_property_train_data, shuffle=True,\n","                                  batch_size=batch_size)\n","    # DataLoader 를 통해 위에서 만든 entity_property_dev_data 를 배치 사이즈 수 단위로 나누고, 데이터를 셔플\n","    entity_property_dev_dataloader = DataLoader(entity_property_dev_data, shuffle=True,\n","                                batch_size=batch_size)\n","\n","    # DataLoader 를 통해 위에서 만든 polarity_train_data 를 배치 사이즈 수 단위로 나누고, 데이터를 셔플\n","    polarity_train_dataloader = DataLoader(polarity_train_data, shuffle=True,\n","                                                  batch_size=batch_size)\n","    # DataLoader 를 통해 위에서 만든 polarity_dev_data 를 배치 사이즈 수 단위로 나누고, 데이터를 셔플\n","    polarity_dev_dataloader = DataLoader(polarity_dev_data, shuffle=True,\n","                                                batch_size=batch_size)\n","\n","    # 사용할 모델 불러오기\n","    print('loading model')\n","    entity_property_model = RoBertaBaseClassifier(len(tf_id_to_name), len(tokenizer))\n","    # 학습된 pt값에 이어서 학습할 시 아래 주석 제거\n","    # entity_property_model.load_state_dict(torch.load(test_category_extraction_model_path, map_location=device))\n","    # 그래픽 카드 사용\n","    entity_property_model.to(device)\n","\n","    polarity_model = RoBertaBaseClassifier(len(polarity_id_to_name), len(tokenizer))\n","    # 학습된 pt값에 이어서 학습할 시 아래 주석 제거\n","    # polarity_model.load_state_dict(torch.load(test_polarity_classification_model_path, map_location=device))\n","    # 그래픽 카드 사용\n","    polarity_model.to(device)\n","\n","\n","    print('end loading')\n","\n","    # entity_property_optimizer 매개변수 분리\n","    FULL_FINETUNING = True\n","    if FULL_FINETUNING:\n","        entity_property_param_optimizer = list(entity_property_model.named_parameters())\n","        no_decay = ['bias', 'gamma', 'beta']\n","        entity_property_optimizer_grouped_parameters = [\n","            {'params': [p for n, p in entity_property_param_optimizer if not any(nd in n for nd in no_decay)],\n","             'weight_decay_rate': 0.01},\n","            {'params': [p for n, p in entity_property_param_optimizer if any(nd in n for nd in no_decay)],\n","             'weight_decay_rate': 0.0}\n","        ]\n","    else:\n","        entity_property_param_optimizer = list(entity_property_model.classifier.named_parameters())\n","        entity_property_optimizer_grouped_parameters = [{\"params\": [p for n, p in entity_property_param_optimizer]}]\n","\n","    # entity_property_optimizer_setting\n","    entity_property_optimizer = AdamW(\n","        entity_property_optimizer_grouped_parameters,\n","        lr=learning_rate,\n","        eps=eps\n","    )\n","    epochs = num_train_epochs\n","    max_grad_norm = 1.0\n","    total_steps = epochs * len(entity_property_train_dataloader)\n","\n","    # entity_property_scheduler_setting\n","    entity_property_scheduler = get_linear_schedule_with_warmup(\n","        entity_property_optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=total_steps\n","    )\n","\n","    # polarity_model_optimizer 매개변수 분리\n","    if FULL_FINETUNING:\n","        polarity_param_optimizer = list(polarity_model.named_parameters())\n","        no_decay = ['bias', 'gamma', 'beta']\n","        polarity_optimizer_grouped_parameters = [\n","            {'params': [p for n, p in polarity_param_optimizer if not any(nd in n for nd in no_decay)],\n","             'weight_decay_rate': 0.01},\n","            {'params': [p for n, p in polarity_param_optimizer if any(nd in n for nd in no_decay)],\n","             'weight_decay_rate': 0.0}\n","        ]\n","    else:\n","        polarity_param_optimizer = list(polarity_model.classifier.named_parameters())\n","        polarity_optimizer_grouped_parameters = [{\"params\": [p for n, p in polarity_param_optimizer]}]\n","\n","    # polarity_optimizer_setting\n","    polarity_optimizer = AdamW(\n","        polarity_optimizer_grouped_parameters,\n","        lr=learning_rate,\n","        eps=eps\n","    )\n","    epochs = num_train_epochs\n","    max_grad_norm = 1.0\n","    total_steps = epochs * len(polarity_train_dataloader)\n","\n","    # polarity_scheduler_setting\n","    polarity_scheduler = get_linear_schedule_with_warmup(\n","        polarity_optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=total_steps\n","    )\n","\n","    # 모델이 위에서 설정 한 에폭수 만큼 돌아감\n","    epoch_step = 0\n","\n","    for _ in trange(epochs, desc=\"Epoch\"):\n","        entity_property_model.train()\n","        epoch_step += 1\n","\n","        # entity_property train\n","        entity_property_total_loss = 0\n","\n","        for step, batch in enumerate(entity_property_train_dataloader):\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","\n","            entity_property_model.zero_grad()\n","\n","            loss, _ = entity_property_model(b_input_ids, b_input_mask, b_labels)\n","\n","            loss.backward()\n","\n","            entity_property_total_loss += loss.item()\n","\n","            torch.nn.utils.clip_grad_norm_(parameters=entity_property_model.parameters(), max_norm=max_grad_norm)\n","            entity_property_optimizer.step()\n","            entity_property_scheduler.step()\n","\n","        # loss 값 계산\n","        avg_train_loss = entity_property_total_loss / len(entity_property_train_dataloader)\n","        print(\"Entity_Property_Epoch: \", epoch_step)\n","        print(\"Average train loss: {}\".format(avg_train_loss))\n","\n","        # 미리 설정한 경로에 모델 저장\n","        model_saved_path = category_extraction_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n","        torch.save(entity_property_model.state_dict(), model_saved_path)\n","\n","        # do_eval의 설정에 따라 evaluation 수행\n","        if do_eval:\n","            entity_property_model.eval()\n","\n","            pred_list = []\n","            label_list = []\n","\n","            for batch in entity_property_dev_dataloader:\n","                batch = tuple(t.to(device) for t in batch)\n","                b_input_ids, b_input_mask, b_labels = batch\n","\n","                with torch.no_grad():\n","                    loss, logits = entity_property_model(b_input_ids, b_input_mask, b_labels)\n","\n","                predictions = torch.argmax(logits, dim=-1)\n","                pred_list.extend(predictions)\n","                label_list.extend(b_labels)\n","\n","            evaluation(label_list, pred_list, len(tf_id_to_name))\n","\n","\n","        # polarity train\n","        polarity_total_loss = 0\n","\n","        for step, batch in enumerate(polarity_train_dataloader):\n","            batch = tuple(t.to(device) for t in batch)\n","            b_input_ids, b_input_mask, b_labels = batch\n","\n","            polarity_model.zero_grad()\n","\n","            loss, _ = polarity_model(b_input_ids, b_input_mask, b_labels)\n","\n","            loss.backward()\n","\n","            polarity_total_loss += loss.item()\n","\n","\n","            torch.nn.utils.clip_grad_norm_(parameters=polarity_model.parameters(), max_norm=max_grad_norm)\n","            polarity_optimizer.step()\n","            polarity_scheduler.step()\n","\n","        # loss 값 계산\n","        avg_train_loss = polarity_total_loss / len(polarity_train_dataloader)\n","        print(\"Entity_Property_Epoch: \", epoch_step)\n","        print(\"Average train loss: {}\".format(avg_train_loss))\n","\n","        # 미리 설정한 경로에 모델 저장\n","        model_saved_path = polarity_classification_model_path + 'saved_model_epoch_' + str(epoch_step) + '.pt'\n","        torch.save(polarity_model.state_dict(), model_saved_path)\n","\n","        # do_eval의 설정에 따라 evaluation 수행\n","        if do_eval:\n","            polarity_model.eval()\n","\n","            pred_list = []\n","            label_list = []\n","\n","            for batch in polarity_dev_dataloader:\n","                batch = tuple(t.to(device) for t in batch)\n","                b_input_ids, b_input_mask, b_labels = batch\n","\n","                with torch.no_grad():\n","                    loss, logits = polarity_model(b_input_ids, b_input_mask, b_labels)\n","\n","                predictions = torch.argmax(logits, dim=-1)\n","                pred_list.extend(predictions)\n","                label_list.extend(b_labels)\n","\n","            evaluation(label_list, pred_list, len(polarity_id_to_name))\n","\n","    print(\"training is done\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2K7iPMzFjdWO"},"outputs":[],"source":["# 모델 학습 함수 실행\n","train_sentiment_analysis()"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"display_name":"Python 3.9.12 ('tf27')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"vscode":{"interpreter":{"hash":"7d0180d1c4847767d50da3e625e220ba049bf958976222b49356513dbf074996"}}},"nbformat":4,"nbformat_minor":0}